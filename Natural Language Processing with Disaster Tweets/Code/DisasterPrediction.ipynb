{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_csv('../Datasets/train.csv')\n",
    "test_dataset = pd.read_csv('../Datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.drop(['keyword','id','location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords  = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lm = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(value):\n",
    "  value = re.sub(\"[^a-zA-Z]\", \" \", value)\n",
    "  value = re.sub(\" +\", \" \", value)\n",
    "  return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemSentences(sentence):\n",
    "  sentence = cleanData(sentence)\n",
    "  wordsList = list()\n",
    "  sentence = sentence.lower()\n",
    "  wordsList = sentence.split(\" \")\n",
    "  stemmedWords = [ps.stem(x) for x in wordsList if not x in stopwords]\n",
    "  return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemamtizeSentence(sentence):\n",
    "  sentence = cleanData(sentence)\n",
    "  wordsList = list()\n",
    "  sentence = sentence.lower()\n",
    "  wordsList = sentence.split(\" \")\n",
    "  stemmedWords = [lm.lemmatize(x) for x in wordsList if not x in stopwords]\n",
    "  return \" \".join(stemmedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemDataset(dataset):\n",
    "  dataset['Stemmed Text'] = dataset['text'].apply(stemSentences)\n",
    "  dataset['Lemmatized Text'] = dataset['text'].apply(lemamtizeSentence)\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = stemDataset(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Stemmed Text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3       peopl receiv wildfir evacu order california    \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                     Lemmatized Text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked shelter place notified officer ...  \n",
       "3   people receive wildfire evacuation order cali...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Stemmed Text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3       peopl receiv wildfir evacu order california    \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                     Lemmatized Text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked shelter place notified officer ...  \n",
       "3   people receive wildfire evacuation order cali...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [our, deeds, are, the, reason, of, this, earth...\n",
       "1           [forest, fire, near, la, ronge, sask, canada]\n",
       "2       [all, residents, asked, to, shelter, in, place...\n",
       "3       [people, receive, wildfires, evacuation, order...\n",
       "4       [just, got, sent, this, photo, from, ruby, ala...\n",
       "                              ...                        \n",
       "7608    [two, giant, cranes, holding, bridge, collapse...\n",
       "7609    [aria_ahrary, thetawniest, the, out, of, contr...\n",
       "7610    [utc, km, of, volcano, hawaii, http, co, zdtoy...\n",
       "7611    [police, investigating, after, an, bike, colli...\n",
       "7612    [the, latest, more, homes, razed, by, northern...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_data = dataset['text'].apply(gensim.utils.simple_preprocess)\n",
    "w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['W2V text'] = w2v_data.apply(lambda x : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "      <th>W2V text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Stemmed Text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3       peopl receiv wildfir evacu order california    \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                     Lemmatized Text  \\\n",
       "0         deed reason earthquake may allah forgive u   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3   people receive wildfire evacuation order cali...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                            W2V text  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  people receive wildfires evacuation orders in ...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) TFIDF - Naive Bayes - On Lemmatized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,1))\n",
    "tokenized = tfidf.fit_transform(dataset['Lemmatized Text']).toarray()\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tfidf = tokenized.copy()\n",
    "#y_tfidf = train_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_Val, y_train, y_val  = train_test_split(tokenized, train_dataset['target'], test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training Data\n",
      "0.9097391630699944\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.92      3024\n",
      "           1       0.97      0.82      0.89      2305\n",
      "\n",
      "    accuracy                           0.91      5329\n",
      "   macro avg       0.92      0.90      0.91      5329\n",
      "weighted avg       0.92      0.91      0.91      5329\n",
      "\n",
      "[[2963   61]\n",
      " [ 420 1885]]\n",
      "============================================\n",
      "\n",
      "On Validation Data\n",
      "0.7985989492119089\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.91      0.84      1318\n",
      "           1       0.84      0.65      0.73       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.81      0.78      0.78      2284\n",
      "weighted avg       0.80      0.80      0.79      2284\n",
      "\n",
      "[[1200  118]\n",
      " [ 342  624]]\n"
     ]
    }
   ],
   "source": [
    "mnb_tfidf = MultinomialNB(alpha=1)\n",
    "mnb_tfidf.fit(X_train, y_train)\n",
    "\n",
    "train_pred = mnb_tfidf.predict(X_train)\n",
    "val_pred = mnb_tfidf.predict(X_Val)\n",
    "\n",
    "print('On Training Data')\n",
    "print(accuracy_score(y_train, train_pred))\n",
    "print(classification_report(y_train, train_pred))\n",
    "print(confusion_matrix(y_train, train_pred))\n",
    "\n",
    "print('============================================')\n",
    "print()\n",
    "\n",
    "print('On Validation Data')\n",
    "print(accuracy_score(y_val, val_pred))\n",
    "print(classification_report(y_val, val_pred))\n",
    "print(confusion_matrix(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Count Vectorizer - Naive Bayes - On gensim processed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(ngram_range = (1,1), binary=True, analyzer='word')\n",
    "tokenized = bow.fit_transform(dataset['W2V text']).toarray()\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_bow = tokenized.copy()\n",
    "#y_bow = train_dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow, X_Val_bow, y_train_bow, y_val_bow  = train_test_split(tokenized, train_dataset['target'], test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 22051)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training Data\n",
      "0.9172452617751924\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.93      3024\n",
      "           1       0.95      0.85      0.90      2305\n",
      "\n",
      "    accuracy                           0.92      5329\n",
      "   macro avg       0.92      0.91      0.91      5329\n",
      "weighted avg       0.92      0.92      0.92      5329\n",
      "\n",
      "[[2925   99]\n",
      " [ 342 1963]]\n",
      "============================================\n",
      "\n",
      "On Validation Data\n",
      "0.808231173380035\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.87      0.84      1318\n",
      "           1       0.80      0.73      0.76       966\n",
      "\n",
      "    accuracy                           0.81      2284\n",
      "   macro avg       0.81      0.80      0.80      2284\n",
      "weighted avg       0.81      0.81      0.81      2284\n",
      "\n",
      "[[1142  176]\n",
      " [ 262  704]]\n"
     ]
    }
   ],
   "source": [
    "mnb_bow = MultinomialNB(alpha=1)\n",
    "mnb_bow.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "train_pred = mnb_bow.predict(X_train_bow)\n",
    "val_pred = mnb_bow.predict(X_Val_bow)\n",
    "\n",
    "print('On Training Data')\n",
    "print(accuracy_score(y_train_bow, train_pred))\n",
    "print(classification_report(y_train_bow, train_pred))\n",
    "print(confusion_matrix(y_train_bow, train_pred))\n",
    "\n",
    "print('============================================')\n",
    "print()\n",
    "\n",
    "print('On Validation Data')\n",
    "print(accuracy_score(y_val_bow, val_pred))\n",
    "print(classification_report(y_val_bow, val_pred))\n",
    "print(confusion_matrix(y_val_bow, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) TFIDF - Naive Bayes - On gensim processed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v = Word2Vec(window=10, min_count=2, workers=10)\n",
    "# w2v.build_vocab(w2v_data, progress_per=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_w2v = TfidfVectorizer(ngram_range=(1,1))\n",
    "vectorized_w2v_data = tfidf_w2v.fit_transform(dataset['W2V text']).toarray()\n",
    "vectorized_w2v_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v, X_Val_w2v, y_train_w2v, y_val_w2v  = train_test_split(vectorized_w2v_data, train_dataset['target'], test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On Training Data\n",
      "0.8911615687746294\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91      3024\n",
      "           1       0.98      0.77      0.86      2305\n",
      "\n",
      "    accuracy                           0.89      5329\n",
      "   macro avg       0.91      0.88      0.89      5329\n",
      "weighted avg       0.90      0.89      0.89      5329\n",
      "\n",
      "[[2980   44]\n",
      " [ 536 1769]]\n",
      "============================================\n",
      "\n",
      "On Validation Data\n",
      "0.8029772329246935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.94      0.85      1318\n",
      "           1       0.88      0.62      0.73       966\n",
      "\n",
      "    accuracy                           0.80      2284\n",
      "   macro avg       0.82      0.78      0.79      2284\n",
      "weighted avg       0.82      0.80      0.80      2284\n",
      "\n",
      "[[1236   82]\n",
      " [ 368  598]]\n"
     ]
    }
   ],
   "source": [
    "mnb_w2v = MultinomialNB(alpha=1)\n",
    "mnb_w2v.fit(X_train_w2v, y_train_w2v)\n",
    "\n",
    "train_pred = mnb_w2v.predict(X_train_w2v)\n",
    "val_pred = mnb_w2v.predict(X_Val_w2v)\n",
    "\n",
    "print('On Training Data')\n",
    "print(accuracy_score(y_train_w2v, train_pred))\n",
    "print(classification_report(y_train_w2v, train_pred))\n",
    "print(confusion_matrix(y_train_w2v, train_pred))\n",
    "\n",
    "print('============================================')\n",
    "print()\n",
    "\n",
    "print('On Validation Data')\n",
    "print(accuracy_score(y_val_w2v, val_pred))\n",
    "print(classification_report(y_val_w2v, val_pred))\n",
    "print(confusion_matrix(y_val_w2v, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Count Vectorizer - XGBoost Classifier - On gensim processed text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "train_pred = xgb.predict(X_train_bow)\n",
    "val_pred = xgb.predict(X_Val_bow)\n",
    "\n",
    "print('On Training Data')\n",
    "print(accuracy_score(y_train_bow, train_pred))\n",
    "print(classification_report(y_train_bow, train_pred))\n",
    "print(confusion_matrix(y_train_bow, train_pred))\n",
    "\n",
    "print('============================================')\n",
    "print()\n",
    "\n",
    "print('On Validation Data')\n",
    "print(accuracy_score(y_val_bow, val_pred))\n",
    "print(classification_report(y_val_bow, val_pred))\n",
    "print(confusion_matrix(y_val_bow, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyper-parameter tuning xgb model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'use_label_encoder': True,\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': None,\n",
       " 'enable_categorical': False,\n",
       " 'gamma': None,\n",
       " 'gpu_id': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'num_parallel_tree': None,\n",
       " 'predictor': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': None,\n",
       " 'reg_lambda': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': None,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': None}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {\n",
    " 'colsample_bylevel': [0.5],\n",
    " 'colsample_bytree': [0.5],\n",
    " 'learning_rate': [0.05],\n",
    " 'max_depth': [11],\n",
    " 'min_child_weight': [5],\n",
    " 'n_estimators': [50],\n",
    " 'n_jobs': [6],\n",
    " 'random_state': [42],\n",
    " 'reg_alpha': [0],\n",
    " 'reg_lambda': [1],\n",
    " 'subsample': [0.8],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_absolute_percentage_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'top_k_accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=xgb, param_grid=params_grid, scoring='accuracy', n_jobs=10, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train_bow, y_train_bow)\n",
    "\n",
    "train_pred = grid_search.predict(X_train_bow)\n",
    "val_pred = grid_search.predict(X_Val_bow)\n",
    "\n",
    "print('On Training Data')\n",
    "print(accuracy_score(y_train_bow, train_pred))\n",
    "print(classification_report(y_train_bow, train_pred))\n",
    "print(confusion_matrix(y_train_bow, train_pred))\n",
    "\n",
    "print('============================================')\n",
    "print()\n",
    "\n",
    "print('On Validation Data')\n",
    "print(accuracy_score(y_val_bow, val_pred))\n",
    "print(classification_report(y_val_bow, val_pred))\n",
    "print(confusion_matrix(y_val_bow, val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Stemmed Text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3       peopl receiv wildfir evacu order california    \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                     Lemmatized Text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked shelter place notified officer ...  \n",
       "3   people receive wildfire evacuation order cali...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_logReg = dataset['Lemmatized Text']\n",
    "y_logReg = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_logReg = TfidfVectorizer()\n",
    "X_logReg_vectorized = tfidf_logReg.fit_transform(X_logReg).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_logReg , X_Val_logReg, y_train_logReg , y_Val_logReg = train_test_split(X_logReg_vectorized, y_logReg, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logReg = LogisticRegression(solver='sag', C=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Log_loss score is -0.44988765057825\n"
     ]
    }
   ],
   "source": [
    "cv_loss = np.mean(cross_val_score(logReg, X_train_logReg, y_train_logReg, cv=10, scoring='neg_log_loss'))\n",
    "print('CV Log_loss score is {}'.format(cv_loss))\n",
    "\n",
    "cv_score = np.mean(cross_val_score(logReg, X_train_logReg, y_train_logReg, cv=10, scoring='accuracy'))\n",
    "print('CV Accuracy score is {}'.format(cv_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ROC_AUC score 0.7708823290892872\n",
      "\n",
      "Accuracy:  0.7852921864740644\n",
      "[[759 115]\n",
      " [212 437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.87      0.82       874\n",
      "           1       0.79      0.67      0.73       649\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.79      0.77      0.78      1523\n",
      "weighted avg       0.79      0.79      0.78      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logReg.fit(X_train_logReg, y_train_logReg)\n",
    "\n",
    "y_pred_val = logReg.predict(X_Val_logReg)\n",
    "y_pred_val_prob = logReg.predict_proba(X_Val_logReg)\n",
    "auc_score = roc_auc_score(y_Val_logReg, y_pred_val)\n",
    "\n",
    "print(\"CV ROC_AUC score {}\".format(auc_score))\n",
    "print('Accuracy: ', accuracy_score(y_Val_logReg, y_pred_val))\n",
    "    \n",
    "print(confusion_matrix(y_Val_logReg, y_pred_val))\n",
    "print(classification_report(y_Val_logReg, y_pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>Stemmed Text</th>\n",
       "      <th>Lemmatized Text</th>\n",
       "      <th>W2V text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la rong sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>peopl receiv wildfir evacu order california</td>\n",
       "      <td>people receive wildfire evacuation order cali...</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target  \\\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1             Forest fire near La Ronge Sask. Canada       1   \n",
       "2  All residents asked to 'shelter in place' are ...       1   \n",
       "3  13,000 people receive #wildfires evacuation or...       1   \n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "\n",
       "                                        Stemmed Text  \\\n",
       "0          deed reason earthquak may allah forgiv us   \n",
       "1               forest fire near la rong sask canada   \n",
       "2  resid ask shelter place notifi offic evacu she...   \n",
       "3       peopl receiv wildfir evacu order california    \n",
       "4  got sent photo rubi alaska smoke wildfir pour ...   \n",
       "\n",
       "                                     Lemmatized Text  \\\n",
       "0         deed reason earthquake may allah forgive u   \n",
       "1              forest fire near la ronge sask canada   \n",
       "2  resident asked shelter place notified officer ...   \n",
       "3   people receive wildfire evacuation order cali...   \n",
       "4  got sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "                                            W2V text  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  people receive wildfires evacuation orders in ...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0              deed reason earthquake may allah forgive u\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       resident asked shelter place notified officer ...\n",
       "3        people receive wildfire evacuation order cali...\n",
       "4       got sent photo ruby alaska smoke wildfire pour...\n",
       "                              ...                        \n",
       "7608    two giant crane holding bridge collapse nearby...\n",
       "7609     aria ahrary thetawniest control wild fire cal...\n",
       "7610             utc km volcano hawaii http co zdtoyd ebj\n",
       "7611    police investigating e bike collided car littl...\n",
       "7612    latest home razed northern california wildfire...\n",
       "Name: Lemmatized Text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['Lemmatized Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this earthquake ma...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       all residents asked to shelter in place are be...\n",
       "3       people receive wildfires evacuation orders in ...\n",
       "4       just got sent this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse into ...\n",
       "7609    aria_ahrary thetawniest the out of control wil...\n",
       "7610          utc km of volcano hawaii http co zdtoyd ebj\n",
       "7611    police investigating after an bike collided wi...\n",
       "7612    the latest more homes razed by northern califo...\n",
       "Name: W2V text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['W2V text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting max len of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxWordList(sentence):\n",
    "  wordList = sentence.split()\n",
    "  count = len(wordList)\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = dataset['W2V text'].apply(maxWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM - START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LSTM = dataset['W2V text']\n",
    "y_LSTM = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_size = 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One_hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_repr=[one_hot(sentence,voc_size)for sentence in X_LSTM] \n",
    "#onehot_repr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length=35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ... 12580  1570   435]\n",
      " [    0     0     0 ... 11933   313  7896]\n",
      " [    0     0     0 ...  2735 11522  1851]\n",
      " ...\n",
      " [    0     0     0 ...  5289  5597  4642]\n",
      " [    0     0     0 ...  4729  7903  2873]\n",
      " [    0     0     0 ...  5289  4842  6940]]\n"
     ]
    }
   ],
   "source": [
    "embedded_docs = pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0, 11360,  2333, 11522,  2960, 11826,\n",
       "       14741,  4732,  7349,  8534, 12952, 12580,  1570,   435])"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_features=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "\n",
    "# model=Sequential()\n",
    "# model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(100))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "#print(model1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, (7613,))"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs),y_LSTM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_LSTM_final = np.array(embedded_docs)\n",
    "y_LSTM_final = np.array(y_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7613, 35), (7613,))"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_LSTM_final.shape, y_LSTM_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_LSTM, X_test_LSTM, y_train_LSTM, y_test_LSTM = train_test_split(X_LSTM_final, y_LSTM_final, test_size=.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "84/84 [==============================] - 21s 188ms/step - loss: 0.5496 - accuracy: 0.7168\n",
      "Epoch 2/5\n",
      "84/84 [==============================] - 18s 210ms/step - loss: 0.2881 - accuracy: 0.8840\n",
      "Epoch 3/5\n",
      "84/84 [==============================] - 18s 211ms/step - loss: 0.1467 - accuracy: 0.9475\n",
      "Epoch 4/5\n",
      "84/84 [==============================] - 18s 219ms/step - loss: 0.0728 - accuracy: 0.9747\n",
      "Epoch 5/5\n",
      "84/84 [==============================] - 18s 215ms/step - loss: 0.0385 - accuracy: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c995898b50>"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_LSTM,y_train_LSTM,epochs=5,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data\n",
      "167/167 [==============================] - 14s 71ms/step - loss: 0.0163 - accuracy: 0.9951\n",
      "\n",
      "Validation Data\n",
      "72/72 [==============================] - 5s 72ms/step - loss: 0.9630 - accuracy: 0.7548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9629634022712708, 0.7548161149024963]"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train Data')\n",
    "model.evaluate(X_train_LSTM, y_train_LSTM)\n",
    "print()\n",
    "print('Validation Data')\n",
    "model.evaluate(X_test_LSTM, y_test_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73      1318\n",
      "           1       0.00      0.00      0.00       966\n",
      "\n",
      "    accuracy                           0.58      2284\n",
      "   macro avg       0.29      0.50      0.37      2284\n",
      "weighted avg       0.33      0.58      0.42      2284\n",
      "\n",
      "0.5770577933450087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Priyesh Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Priyesh Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Priyesh Dave\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test_LSTM)\n",
    "classes_x=np.argmax(y_pred,axis=1)\n",
    "\n",
    "print(classification_report(y_test_LSTM, classes_x))\n",
    "print(accuracy_score(y_test_LSTM, classes_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\PRIYES~1\\AppData\\Local\\Temp/ipykernel_988/857347703.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pn' is not defined"
     ]
    }
   ],
   "source": [
    "pn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processTestData(dataset, vectorizer):\n",
    "  #dataset.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "  transformed_data = vectorizer.transform(dataset['text']).toarray()\n",
    "  return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictions(dataset, vectorizer, model):\n",
    "  transformed_data = processTestData(dataset, vectorizer)\n",
    "  test_pred = model.predict(transformed_data)\n",
    "  return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dataset, vectorizer, model, filename):\n",
    "  # Getting predictions on test data.\n",
    "  test_pred = getPredictions(dataset, vectorizer, model)\n",
    "\n",
    "  #Creating predictions dataframe.\n",
    "  predictions_df = pd.DataFrame(test_pred, columns=['target'])\n",
    "  predictions_df = pd.concat([test_dataset['id'], predictions_df], axis=1)\n",
    "\n",
    "  #Saving the predictions.\n",
    "  predictions_df.to_csv('../Datasets/{}.csv'.format(filename), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for tfidf\n",
    "predict(test_dataset, tfidf, mnb_tfidf, 'predictions_tfidf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for bow\n",
    "#predict(test_dataset, bow, mnb_bow, 'predictions_bow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting predictions for bow with w2v data.\n",
    "predict(test_dataset, bow, mnb_bow, 'predictions_bow_w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Model/mnb_bow_w2v.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(value=mnb_bow, filename='../Model/mnb_bow_w2v.pkl')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e147b052be5536a019399d7281fa07f9aa68d0dfdf21ead313763d9c98e78705"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
